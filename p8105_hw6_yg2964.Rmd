---
title: "p8105_hw6_yg2964"
output: github_document
date: "2024-12-02"
---

```{r}
library(tidyverse)
library(modelr)
library(ggplot2)
library(mgcv)
library(broom)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(127)
```

### Problem 1

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") |> 
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |> 
  select(name, id, everything())

get_resample = function(df) {
  sample_frac(df, replace = TRUE)
}

extract_metrics = function(model) {
  intercept = tidy(model) |> filter(term == "(Intercept)") |> pull(estimate)
  slope = tidy(model) |> filter(term == "tmin") |> pull(estimate)
  
  tibble(
    rsquared = glance(model)$r.squared,
    coef_product = log(abs(intercept * slope))
  )
}

resampling_sets = 
  tibble(rep = 1:5000) |>
  mutate(
    resamples = map(rep, ~get_resample(weather_df))
  )

resample_results = 
  resampling_sets |>
  mutate(
    fits = map(resamples, ~lm(tmax ~ tmin, data = .)),
    metrics = map(fits, extract_metrics)
  ) |>
  select(-resamples, -fits) |>
  unnest(metrics)

# Get confidence bounds
conf_intervals = resample_results |>
  summarize(across(everything(), 
                  list(lower = ~quantile(., 0.025),
                       upper = ~quantile(., 0.975))))

# Create visualizations
viz1 = resample_results |>
  ggplot(aes(x = rsquared)) +
  geom_histogram(fill = "seagreen3", color = "seagreen4", alpha = 0.8) +
  labs(title = "R² Distribution", 
       x = "R-squared Value", 
       y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))

viz2 = resample_results |>
  ggplot(aes(x = coef_product)) +
  geom_histogram(fill = "orchid3", color = "orchid4", alpha = 0.8) +
  labs(title = "log(β₀*β₁) Distribution", 
       x = "Log Product of Coefficients", 
       y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))

viz1
viz2
```

R² Distribution:
Centered around 0.91 with values ranging from 0.88 to 0.94, showing a compact and symmetric distribution that indicates a strong and stable linear relationship between tmax and tmin.
log(β₀*β₁) Distribution:
Centered around 2.02 with values ranging from 1.95 to 2.10, displaying a symmetric distribution that confirms stable, positive coefficient products and supports the positive correlation between variables.


### Problem 2
```{r}
crime_data = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(
    location = paste(city, state, sep = ", "),
    case_solved = ifelse(disposition == "Closed by arrest", 1, 0),
    age = as.numeric(victim_age)
  ) |>
  filter(!location %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) |>
  filter(victim_race %in% c("White", "Black"))
```

#### Single City Analysis (Baltimore)
```{r}
baltimore_data = crime_data |>
  filter(location == "Baltimore, MD") |>
  select(uid, case_solved, age, victim_sex, victim_race)

# Fit logistic model for Baltimore
fit_md_logistic = baltimore_data |>
  glm(case_solved ~ age + victim_sex + victim_race,
      data = _, family = binomial())

# Get Baltimore results
fit_md_logistic |>
  broom::tidy(conf.int = TRUE) |>
  mutate(
    OR = exp(estimate),
    OR_conf.low = exp(conf.low),
    OR_conf.high = exp(conf.high)
  ) |>
  filter(term == "victim_sexMale") |>
  select(term, OR, OR_conf.low, OR_conf.high) |>
  knitr::kable(digits = 3)


```
The odds of solving a homicide in Baltimore for male victims are 0.426 times the odds for female victims (95% CI: 0.324-0.558), after adjusting for age and race. This indicates significantly lower clearance rates for male victims compared to female victims.

####  Analysis for All Cities
```{r}
# Function for City Analysis 
fit_logistic = function(data){
  model = glm(case_solved ~ age + victim_sex + victim_race,
              data = data, family = binomial())
  
  model |>
    broom::tidy(conf.int = TRUE) |>
    mutate(
      OR = exp(estimate),
      OR_conf.low = exp(conf.low),
      OR_conf.high = exp(conf.high)
    ) |>
    filter(term == "victim_sexMale") |>
    select(term, OR, OR_conf.low, OR_conf.high)
}
city_results = crime_data |>
  group_by(location) |>
  nest() |>
  mutate(
    model = map(data, fit_logistic)
  ) |>
  unnest(model) |>
  select(location, OR, OR_conf.low, OR_conf.high) |>
  ungroup()


city_results |>
  knitr::kable(digits = 3)


```

#### Create Plot
```{r}
city_results |>
  mutate(
    location = fct_reorder(location, OR)
  ) |>
  ggplot(aes(x = location, y = OR)) +
  geom_point() +
  geom_errorbar(
    aes(ymin = OR_conf.low, ymax = OR_conf.high), 
    width = 0.5
  ) +
  geom_hline(
    yintercept = 1, 
    linetype = "dashed", 
    color = "red"
  ) +
  labs(
    title = "Adjusted Odds Ratios for Solving Homicides by City",
    x = "City",
    y = "Odds Ratio (Male vs Female)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
  )
```

The plot shows adjusted odds ratios for solving homicides comparing male to female victims across different U.S. cities. A few key observations:

- Most cities show odds ratios below 1 (red dashed line), indicating that homicides with male victims are generally less likely to be solved than those with female victims.

- Cities like New York and Baton Rouge have the lowest odds ratios (around 0.3-0.4), suggesting male victim cases are much less likely to be solved in these locations.

- A few cities like Albuquerque and Stockton show odds ratios above 1, indicating higher likelihood of solving male victim cases, though their wide confidence intervals suggest less precise estimates.

- The confidence intervals (vertical lines) vary in width across cities, reflecting different levels of uncertainty in the estimates, with smaller cities typically showing wider intervals.

- Baltimore (shown earlier with OR=0.426) falls in the lower third of cities, indicating a relatively strong disparity in solving cases between male and female victims.

### Problem 3
```{r}
birthweight = read.csv("data/birthweight.csv", na = c("NA","",".")) |>
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), 
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8),
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  ) |>
  drop_na()
```

```{r}
colSums(is.na(birthweight))
```

#### Create hypothesized model
```{r}
model = lm(bwt ~ gaweeks + bhead + blength + mheight + wtgain + ppbmi + smoken + parity + pnumlbw + babysex,                        
           data = birthweight)

summary(model)

# Create plot
birthweight |>
  add_predictions(model) |>
  add_residuals(model) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values for Birthweight Model",
    x = "Fitted Values (grams)",
    y = "Residuals (grams)"
  ) +
  theme_minimal()
```

#### Compare models
```{r}
cv_df = birthweight |>
  crossv_mc(100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_results = cv_df |>
  mutate(
    fit_bw_0 = map(train, ~lm(bwt ~ gaweeks + bhead + blength + mheight + wtgain + 
                             ppbmi + smoken + parity + pnumlbw + babysex, data = .x)),
    fit_bw_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit_bw_2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) |>
  mutate(
    rmse_fit0 = map2_dbl(fit_bw_0, test, ~rmse(model = .x, data = .y)),
    rmse_fit1 = map2_dbl(fit_bw_1, test, ~rmse(model = .x, data = .y)),
    rmse_fit2 = map2_dbl(fit_bw_2, test, ~rmse(model = .x, data = .y))
  )

# Create visualization
cv_results |>
  select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") |>
  mutate(model = factor(model, 
                       levels = c("fit0", "fit1", "fit2"),
                       labels = c("Full Model", "Length + GA", "Interactions"))) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin(trim = FALSE) +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12)
  ) +
  labs(
    x = "Model",
    y = "RMSE"
  )
```

The Full Model shows the best predictive performance with the lowest RMSE around 280-300 grams. While the Interactions model performs moderately well, the Length + GA model has the highest prediction errors. This suggests that using multiple relevant predictors (Full Model) is more effective than either using just two basic measurements or focusing on complex interactions between variables.